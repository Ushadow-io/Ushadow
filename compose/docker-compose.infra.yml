# ushadow Infrastructure Services
# Persistent database layer that stays running across app restarts
#
# Usage:
#   docker compose -f compose/docker-compose.infra.yml --profile infra up -d  # Start core (mongo, redis, keycloak)
#   docker compose -f compose/docker-compose.infra.yml --profile llm up -d    # Start Ollama
#   docker compose -f compose/docker-compose.infra.yml down                   # Stop (keeps data)
#   docker compose -f compose/docker-compose.infra.yml down -v                # Stop and remove data

# =============================================================================
# USHADOW METADATA (ignored by Docker, read by ushadow backend)
# =============================================================================
x-ushadow:
  ollama:
    display_name: "Ollama"
    description: "Local LLM inference server with OpenAI-compatible API"
    requires: []
    provides: llm
    tags: ["llm", "inference", "local"]

name: infra

services:
  mongo:
    image: mongo:8.0
    container_name: mongo
    profiles: ["infra"]
    ports:
      - "27017:27017"
    environment:
      # Infrastructure exports - what this service provides to consumers
      - MONGODB_HOST=${MONGODB_HOST:-mongo}
      - MONGODB_PORT=${MONGODB_PORT:-27017}
      - MONGODB_DATABASE=${MONGODB_DATABASE:-ushadow}
      - MONGODB_AUTH_SOURCE=${MONGODB_AUTH_SOURCE:-admin}
    volumes:
      - mongo_data:/data/db
    command: ["--replSet", "rs0", "--bind_ip_all"]
    healthcheck:
      test: ["CMD", "mongosh", "--quiet", "--eval", "try { rs.status().ok } catch(e) { 0 }"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    networks:
      - ushadow-network
      - infra-network
    restart: unless-stopped

  # One-time replica set initialization (runs and exits)
  mongo-init:
    image: mongo:8.0
    container_name: mongo-init
    profiles: ["infra"]
    depends_on:
      mongo:
        condition: service_started
    entrypoint: >
      mongosh --host mongo --quiet --eval "
        try {
          rs.status();
          print('Replica set already initialized');
        } catch(e) {
          print('Initializing replica set...');
          rs.initiate({_id: 'rs0', members: [{_id: 0, host: 'mongo:27017'}]});
          print('Replica set initialized');
        }
      "
    networks:
      - infra-network
    restart: "no"

  redis:
    image: redis:7-alpine
    container_name: redis
    profiles: ["infra"]
    ports:
      - "6379:6379"
    environment:
      # Infrastructure exports
      - REDIS_HOST=${REDIS_HOST:-redis}
      - REDIS_PORT=${REDIS_PORT:-6379}
      - REDIS_URL=${REDIS_URL:-redis://redis:6379/0}
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - ushadow-network
      - infra-network
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    profiles: ["memory","qdrant","infra"]
    ports:
      - "6333:6333"  # HTTP
      - "6334:6334"  # gRPC
    environment:
      # Infrastructure exports
      - QDRANT_HOST=${QDRANT_HOST:-qdrant}
      - QDRANT_PORT=${QDRANT_PORT:-6333}
      - QDRANT_URL=${QDRANT_URL:-http://qdrant:6333}
      - QDRANT_GRPC_PORT=${QDRANT_GRPC_PORT:-6334}
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - ushadow-network
      - infra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:6333/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  postgres:
    image: postgres:16-alpine
    container_name: postgres
    profiles: ["memory","metamcp","postgres","infra"]
    ports:
      - "5432:5432"
    environment:
      # Postgres internal config
      - POSTGRES_USER=${POSTGRES_USER:-ushadow}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-ushadow}
      - POSTGRES_DB=${POSTGRES_DB:-ushadow}
      - POSTGRES_MULTIPLE_DATABASES=${POSTGRES_MULTIPLE_DATABASES:-metamcp,openmemory}

      # Infrastructure exports - what this service provides to consumers
      - POSTGRES_HOST=${POSTGRES_HOST:-postgres}
      - POSTGRES_PORT=${POSTGRES_PORT:-5432}
      - DATABASE_URL=${DATABASE_URL:-postgresql://ushadow:ushadow@postgres:5432/ushadow}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ../config/postgres-init:/docker-entrypoint-initdb.d:ro
    networks:
      - ushadow-network
      - infra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-ushadow}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  neo4j:
    image: neo4j:latest
    container_name: neo4j
    profiles: ["memory","neo4j"]
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    environment:
      # Neo4j internal config
      - NEO4J_AUTH=${NEO4J_USERNAME:-neo4j}/${NEO4J_PASSWORD:-password}
      - NEO4J_PLUGINS=["apoc"]

      # Infrastructure exports
      - NEO4J_URI=${NEO4J_URI:-bolt://neo4j:7687}
      - NEO4J_HOST=${NEO4J_HOST:-neo4j}
      - NEO4J_BOLT_PORT=${NEO4J_BOLT_PORT:-7687}
      - NEO4J_HTTP_PORT=${NEO4J_HTTP_PORT:-7474}
    volumes:
      - neo4j_data:/data
    networks:
      - ushadow-network
      - infra-network
    restart: unless-stopped

    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:7474"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  keycloak:
    image: quay.io/keycloak/keycloak:26.0
    container_name: keycloak
    profiles: ["infra"]
    ports:
      - "${KC_PORT:-8081}:8080"
      - "${KC_MGMT_PORT:-9000}:9000"  # Management
    environment:
      # Keycloak internal config (KC_* are Keycloak's native env vars)
      - KEYCLOAK_ADMIN=${KC_BOOTSTRAP_ADMIN_USERNAME:-admin}
      - KEYCLOAK_ADMIN_PASSWORD=${KC_BOOTSTRAP_ADMIN_PASSWORD:-admin}
      - KC_DB=postgres
      - KC_DB_URL=jdbc:postgresql://postgres:5432/${POSTGRES_DB:-ushadow}
      - KC_DB_USERNAME=${POSTGRES_USER:-ushadow}
      - KC_DB_PASSWORD=${POSTGRES_PASSWORD:-ushadow}

      # Hostname v2 configuration (Keycloak 26+)
      # KC_HOSTNAME_STRICT=false allows the backend to call via internal http://keycloak:8080.
      # KC_PROXY_HEADERS=xforwarded lets Keycloak use X-Forwarded-Host/Proto for the issuer,
      # so the backend can spoof the public hostname on internal calls to match the token issuer.
      - KC_HOSTNAME_STRICT=false
      - KC_HTTP_ENABLED=true
      - KC_HEALTH_ENABLED=true
      - KC_PROXY_HEADERS=xforwarded

      # Additional Keycloak config
      - KC_REALM=${KC_REALM:-ushadow}  # Default realm
      # Fix JVM SIGBUS crash on ARM64 (aarch64) â€” disables shared perf memory
      # that causes PerfLongVariant::sample() to segfault in containers (pid=1)
      - JAVA_OPTS_APPEND=-XX:-UsePerfData -XX:+DisableAttachMechanism
    volumes:
      - ../ushadow/frontend/keycloak-theme:/opt/keycloak/themes/ushadow:ro
      - ../config/keycloak:/opt/keycloak/data/import:ro
    command:
      - start-dev
      - --import-realm
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - ushadow-network
      - infra-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    profiles: ["llm", "ollama"]
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama_models:/root/.ollama
    networks:
      - ushadow-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # tailscale:
  #   image: tailscale/tailscale:latest
  #   container_name: ushadow-tailscale
  #   hostname: ushadow-tailscale
  #   environment:
  #     - TS_STATE_DIR=/var/lib/tailscale
  #     - TS_USERSPACE=true
  #     - TS_ACCEPT_DNS=true
  #     - TS_EXTRA_ARGS=--advertise-tags=tag:container
  #   volumes:
  #     - tailscale_state:/var/lib/tailscale
  #     - ./config/SECRETS/certs:/certs
  #   cap_add:
  #     - NET_ADMIN
  #     - NET_RAW
  #   networks:
  #     - ushadow-network
  #   restart: unless-stopped
  #   command: sh -c "tailscaled --tun=userspace-networking --statedir=/var/lib/tailscale & sleep infinity"

networks:
  ushadow-network:
    name: ushadow-network
    external: true
  infra-network:
    name: infra-network
    external: true

volumes:
  mongo_data:
    driver: local
  redis_data:
    driver: local
  qdrant_data:
    driver: local
  postgres_data:
    driver: local
  neo4j_data:
    driver: local
  tailscale_state:
    driver: local
  ollama_models:
    driver: local
